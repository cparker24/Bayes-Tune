{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import dill\n",
    "import pickle\n",
    "sys.path.insert(0, path.abspath('./'))\n",
    "\n",
    "from src.emulator_BAND import EmulatorBAND\n",
    "from src.emulator import Emulator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to create pickle file with training dataset and the second one with the test points\n",
    "\n",
    "Use the last 5 posterior points for the second file and the 1000 LHC points + 95 posterior points for the training of the emulators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset hadrons.pkl has length 500.\n",
      "hadrons_train.pkl has the correct length: 495\n",
      "hadrons_posterior.pkl has the correct length: 5\n",
      "The dataset pions.pkl has length 500.\n",
      "pions_train.pkl has the correct length: 495\n",
      "pions_posterior.pkl has the correct length: 5\n",
      "The dataset kaons.pkl has length 500.\n",
      "kaons_train.pkl has the correct length: 495\n",
      "kaons_posterior.pkl has the correct length: 5\n",
      "The dataset protons.pkl has length 500.\n",
      "protons_train.pkl has the correct length: 495\n",
      "protons_posterior.pkl has the correct length: 5\n"
     ]
    }
   ],
   "source": [
    "path_data = 'preds/'\n",
    "path_output = './output/'\n",
    "datasets = [\n",
    "    'hadrons.pkl',\n",
    "    'pions.pkl',\n",
    "    'kaons.pkl',\n",
    "    'protons.pkl'\n",
    "]\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "def check_file_length(filename, expected_length):\n",
    "    with open(f\"{path_output}{filename}\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    if len(data) == expected_length:\n",
    "        print(f\"{filename} has the correct length: {expected_length}\")\n",
    "    else:\n",
    "        print(f\"{filename} does not have the correct length. Expected: {expected_length}, Actual: {len(data)}\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "    \n",
    "    print(f\"The dataset {dataset} has length {len(data)}.\")\n",
    "    # Separate data based on event ID\n",
    "    sorted_event_ids = sorted(data.keys(), key=lambda x: int(x))\n",
    "    first_data = {event_id: data[event_id] for event_id in sorted_event_ids[:495]}\n",
    "    second_data = {event_id: data[event_id] for event_id in sorted_event_ids[495:500]}\n",
    "\n",
    "    # Save separated data to pickle files\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_train.pkl', 'wb') as pf1:\n",
    "        pickle.dump(first_data, pf1)\n",
    "        \n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_posterior.pkl', 'wb') as pf2:\n",
    "        pickle.dump(second_data, pf2)\n",
    "\n",
    "    check_file_length(f'{dataset.split(\".p\")[0]}_train.pkl', 495)\n",
    "    check_file_length(f'{dataset.split(\".p\")[0]}_posterior.pkl', 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the training of the emulators for all datasets\n",
    "\n",
    "After training the emulators, we save them with `dill`, such that they can be reloaded from file for the MCMC later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training without the `parameterTrafoPCA` (for $\\zeta/s(T)$, $\\eta/s(\\mu_B)$, $\\langle y_{\\rm loss}\\rangle(y_{\\rm init})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][emulator_BAND] loading training data from ./output/hadrons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator_BAND] loading training data from ./output/hadrons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] loading training data from ./output/hadrons_train.pkl ...\n",
      "[INFO][emulator] All training data are loaded.\n",
      "[INFO][emulator] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator] Performing PCA ...\n",
      "[INFO][emulator] 4 PCs explain 0.91426 of variance\n",
      "[INFO][emulator] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] GP 0: 0.53858 of variance, LML = -0.73738, Score = 0.98, kernel: 5.8**2 * RBF(length_scale=[2.61, 6.08, 2.05, 2.57, 19.5, 6.99]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 1: 0.25080 of variance, LML = -298.91, Score = 0.93, kernel: 1.17**2 * RBF(length_scale=[0.592, 0.184, 1.86, 0.611, 1.71, 0.658]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 2: 0.06666 of variance, LML = -214.01, Score = 0.95, kernel: 1.13**2 * RBF(length_scale=[0.567, 0.262, 1.6, 0.435, 3.99, 1.34]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 3: 0.05822 of variance, LML = -328.86, Score = 0.94, kernel: 1.09**2 * RBF(length_scale=[0.567, 0.168, 0.998, 0.311, 1.77, 0.914]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator_BAND] loading training data from ./output/pions_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator_BAND] loading training data from ./output/pions_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] loading training data from ./output/pions_train.pkl ...\n",
      "[INFO][emulator] All training data are loaded.\n",
      "[INFO][emulator] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator] Performing PCA ...\n",
      "[INFO][emulator] 4 PCs explain 0.99413 of variance\n",
      "[INFO][emulator] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] GP 0: 0.53257 of variance, LML = -42.101, Score = 0.98, kernel: 5.82**2 * RBF(length_scale=[1.78, 3.88, 3.98, 4.52, 2.74, 1.04]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 1: 0.40340 of variance, LML = -61.458, Score = 0.96, kernel: 4.19**2 * RBF(length_scale=[3.36, 5.07, 2.35, 6.31, 2.08, 0.915]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 2: 0.04947 of variance, LML = -97.503, Score = 0.96, kernel: 3.47**2 * RBF(length_scale=[1.38, 0.753, 2.36, 1.24, 50, 40]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 3: 0.00869 of variance, LML = -247.36, Score = 0.94, kernel: 1.07**2 * RBF(length_scale=[0.537, 0.246, 1.8, 0.443, 1.5, 1.07]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator_BAND] loading training data from ./output/kaons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator_BAND] loading training data from ./output/kaons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] loading training data from ./output/kaons_train.pkl ...\n",
      "[INFO][emulator] All training data are loaded.\n",
      "[INFO][emulator] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator] Performing PCA ...\n",
      "[INFO][emulator] 4 PCs explain 0.99586 of variance\n",
      "[INFO][emulator] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] GP 0: 0.83234 of variance, LML = -5.1716, Score = 0.98, kernel: 4.25**2 * RBF(length_scale=[7.41, 12.4, 7.74, 8.79, 0.735, 0.973]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 1: 0.12920 of variance, LML = -54.903, Score = 0.97, kernel: 3.6**2 * RBF(length_scale=[3.82, 2.72, 1.37, 1.3, 1.2, 1.8]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 2: 0.02962 of variance, LML = -270.38, Score = 0.93, kernel: 2.06**2 * RBF(length_scale=[0.773, 0.347, 1.49, 0.77, 1.05, 1.79]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 3: 0.00471 of variance, LML = -358.18, Score = 0.94, kernel: 1.2**2 * RBF(length_scale=[0.481, 0.205, 1.27, 0.364, 0.645, 0.97]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator_BAND] loading training data from ./output/protons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator_BAND] loading training data from ./output/protons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] loading training data from ./output/protons_train.pkl ...\n",
      "[INFO][emulator] All training data are loaded.\n",
      "[INFO][emulator] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator] Performing PCA ...\n",
      "[INFO][emulator] 4 PCs explain 0.99750 of variance\n",
      "[INFO][emulator] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] GP 0: 0.92587 of variance, LML = 30.31, Score = 0.99, kernel: 2.77**2 * RBF(length_scale=[5.65, 10.7, 7.12, 30, 0.989, 0.369]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 1: 0.06055 of variance, LML = -29.414, Score = 0.99, kernel: 3.19**2 * RBF(length_scale=[3.68, 3.26, 1.22, 1.24, 1.29, 0.503]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 2: 0.00747 of variance, LML = -244.06, Score = 0.94, kernel: 2.79**2 * RBF(length_scale=[0.938, 1.68, 0.632, 0.771, 2.1, 0.462]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 3: 0.00361 of variance, LML = -198.92, Score = 0.92, kernel: 1.28**2 * RBF(length_scale=[2.07, 0.501, 1.31, 0.666, 0.889, 0.36]) + WhiteKernel(noise_level=0.01)\n"
     ]
    }
   ],
   "source": [
    "model_par = 'preds/modelDesign.txt'\n",
    "path_input = './output/'\n",
    "path_output = './trained_emulators_no_PCA/'\n",
    "\n",
    "datasets_train = [\n",
    "    'hadrons_train.pkl',\n",
    "    'pions_train.pkl',\n",
    "    'kaons_train.pkl',\n",
    "    'protons_train.pkl'\n",
    "]\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets_train:\n",
    "    emu1 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCGP', logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu1.trainEmulatorAutoMask()\n",
    "    emu2 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCSK', logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu2.trainEmulatorAutoMask()\n",
    "    emu3 = Emulator(f'{path_input}{dataset}', model_par, npc = 4, logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu3.trainEmulatorAutoMask()\n",
    "\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCGP_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu1, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCSK_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu2, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_Scikit_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training with the `parameterTrafoPCA` (for $\\zeta/s(T)$, $\\eta/s(\\mu_B)$, $\\langle y_{\\rm loss}\\rangle(y_{\\rm init})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadrons_train.pkl\n",
      "[INFO][emulator_BAND] loading training data from ./output/hadrons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Prepare bulk viscosity parameter PCA ...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 15 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets_train:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dataset)\n\u001b[0;32m---> 18\u001b[0m     emu1 \u001b[38;5;241m=\u001b[39m \u001b[43mEmulatorBAND\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath_input\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_par\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPCGP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogTrafo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameterTrafoPCA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     emu1\u001b[38;5;241m.\u001b[39mtrainEmulatorAutoMask()\n\u001b[1;32m     20\u001b[0m     emu2 \u001b[38;5;241m=\u001b[39m EmulatorBAND(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, model_par, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPCSK\u001b[39m\u001b[38;5;124m'\u001b[39m, logTrafo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, parameterTrafoPCA\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/scratch/user/cameron.parker/projects/Bayes-Tune/src/emulator_BAND.py:57\u001b[0m, in \u001b[0;36mEmulatorBAND.__init__\u001b[0;34m(self, training_set_path, parameter_file, method, logTrafo, parameterTrafoPCA)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparamTrafoPCA_bulk \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargetVariance)\u001b[38;5;66;03m# 0.99 is the minimum of explained variance\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices_zeta_s_parameters \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m17\u001b[39m,\u001b[38;5;241m18\u001b[39m] \u001b[38;5;66;03m# zeta_max,T_zeta0,sigma_plus,sigma_minus\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_bulk_viscosity_PCA\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrepare shear viscosity parameter PCA ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparamTrafoScaler_shear \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[0;32m/scratch/user/cameron.parker/projects/Bayes-Tune/src/emulator_BAND.py:141\u001b[0m, in \u001b[0;36mEmulatorBAND.perform_bulk_viscosity_PCA\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_bulk_viscosity_PCA\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# get the corresponding parameters for the training points\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     bulk_viscosity_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesign_points\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices_zeta_s_parameters\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    142\u001b[0m     T_range \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    143\u001b[0m     data_functions \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mIndexError\u001b[0m: index 15 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "model_par = 'preds/modelDesign.txt'\n",
    "path_input = './output/'\n",
    "path_output = './separate_training_posterior_data/'\n",
    "\n",
    "datasets_train = [\n",
    "    'hadrons_train.pkl',\n",
    "    'pions_train.pkl',\n",
    "    'kaons_train.pkl',\n",
    "    'protons_train.pkl'\n",
    "]\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets_train:\n",
    "    print(dataset)\n",
    "    emu1 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCGP', logTrafo=False, parameterTrafoPCA=True)\n",
    "    emu1.trainEmulatorAutoMask()\n",
    "    emu2 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCSK', logTrafo=False, parameterTrafoPCA=True)\n",
    "    emu2.trainEmulatorAutoMask()\n",
    "    emu3 = Emulator(f'{path_input}{dataset}', model_par, npc = 4, logTrafo=False, parameterTrafoPCA=True)\n",
    "    emu3.trainEmulatorAutoMask()\n",
    "\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCGP_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu1, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCSK_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu2, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_Scikit_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an 'experimental' dataset from one of the posterior points for closure testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[4:5]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1099']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1099']['obs'] = np.concatenate((event_data[0]['1099']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1099.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[2:3]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1097']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1097']['obs'] = np.concatenate((event_data[0]['1097']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1097.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the point for the test of the logarithmic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[4:5]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "# modify the datasets with the multiplicities and take the log\n",
    "datasets_to_modify = [0,1,4,5,7]\n",
    "for i in datasets_to_modify:\n",
    "    event_data[i]['1099']['obs'][0,:] = np.log(np.abs(event_data[i]['1099']['obs'][0,:]) + 1e-30)\n",
    "    event_data[i]['1099']['obs'][1,:] = np.abs(event_data[i]['1099']['obs'][1,:]/event_data[i]['1099']['obs'][0,:] + 1e-30)\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1099']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1099']['obs'] = np.concatenate((event_data[0]['1099']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1099_LOG.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate one dataset from all of the training and posterior points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "path_output = './full_data_one_pkl/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta.pkl',\n",
    "            'AuAu200_PHOBOSv2eta.pkl',\n",
    "            'AuAu200_pTvn.pkl',\n",
    "            'AuAu19p6_dNdy.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta.pkl',\n",
    "            'AuAu19p6_pTvn.pkl',\n",
    "            'AuAu7.7_dNdy.pkl',\n",
    "            'AuAu7.7_pTvn.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    sorted_keys = sorted(data.keys())\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted_keys}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "for dataset in event_data[1:]:\n",
    "    for event in sorted_keys:\n",
    "        # Get the 'obs' array for the current event\n",
    "        obs_array_new = dataset[event]['obs']\n",
    "        \n",
    "        # Extend the 'obs' array of the first dataset with the values from the others\n",
    "        event_data[0][event]['obs'] = np.concatenate((event_data[0][event]['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}all_points_all_observables.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete parameters 16 and 17 from pkl files (bulk_max_rhob2,bulk_max_rhob4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "path_output = '../data_new/'\n",
    "datasets = ['AuAu7.7_dNdy.pkl',\n",
    "            'AuAu7.7_pTvn.pkl',\n",
    "            'AuAu19p6_dNdy.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta.pkl',\n",
    "            'AuAu19p6_pTvn.pkl',\n",
    "            'AuAu200_dNdy.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta.pkl',\n",
    "            'AuAu200_PHOBOSv2eta.pkl',\n",
    "            'AuAu200_pTvn.pkl',\n",
    "            'AuAu7.7_logdNdy.pkl',\n",
    "            'AuAu19p6_logdNdy.pkl',\n",
    "            'AuAu19p6_logPHOBOSdNdeta.pkl',\n",
    "            'AuAu200_logdNdy.pkl',\n",
    "            'AuAu200_logPHOBOSdNdeta.pkl'\n",
    "            ]\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    print(f\"The dataset {dataset} has length {len(data)}.\")\n",
    "    # Separate data based on event ID\n",
    "    sorted_event_ids = sorted(data.keys(), key=lambda x: int(x))\n",
    "    first_data = {event_id: data[event_id] for event_id in sorted_event_ids}\n",
    "    print(\"Parameters before =\",len(first_data[f'{sorted_event_ids[0]}']['parameter']))\n",
    "\n",
    "    for point in range(len(sorted_event_ids)):\n",
    "        first_data[f'{sorted_event_ids[point]}']['parameter'] = np.delete(first_data[f'{sorted_event_ids[point]}']['parameter'], [16,17])\n",
    "\n",
    "    print(\"Parameters after =\",len(first_data[f'{sorted_event_ids[0]}']['parameter']))\n",
    "    # Save new data to pickle files\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}.pkl', 'wb') as pf1:\n",
    "        pickle.dump(first_data, pf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
