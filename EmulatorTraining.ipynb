{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import dill\n",
    "import pickle\n",
    "sys.path.insert(0, path.abspath('./'))\n",
    "\n",
    "from src.emulator_BAND import EmulatorBAND\n",
    "from src.emulator import Emulator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to create pickle file with training dataset and the second one with the test points\n",
    "\n",
    "Use the last 5 posterior points for the second file and the 1000 LHC points + 95 posterior points for the training of the emulators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset hadrons.pkl has length 500.\n",
      "hadrons_train.pkl has the correct length: 495\n",
      "hadrons_posterior.pkl has the correct length: 5\n",
      "The dataset pions.pkl has length 500.\n",
      "pions_train.pkl has the correct length: 495\n",
      "pions_posterior.pkl has the correct length: 5\n",
      "The dataset kaons.pkl has length 500.\n",
      "kaons_train.pkl has the correct length: 495\n",
      "kaons_posterior.pkl has the correct length: 5\n",
      "The dataset protons.pkl has length 500.\n",
      "protons_train.pkl has the correct length: 495\n",
      "protons_posterior.pkl has the correct length: 5\n"
     ]
    }
   ],
   "source": [
    "path_data = 'preds/'\n",
    "path_output = './output/'\n",
    "datasets = [\n",
    "    'hadrons.pkl',\n",
    "    'pions.pkl',\n",
    "    'kaons.pkl',\n",
    "    'protons.pkl'\n",
    "]\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "def check_file_length(filename, expected_length):\n",
    "    with open(f\"{path_output}{filename}\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    if len(data) == expected_length:\n",
    "        print(f\"{filename} has the correct length: {expected_length}\")\n",
    "    else:\n",
    "        print(f\"{filename} does not have the correct length. Expected: {expected_length}, Actual: {len(data)}\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "    \n",
    "    print(f\"The dataset {dataset} has length {len(data)}.\")\n",
    "    # Separate data based on event ID\n",
    "    sorted_event_ids = sorted(data.keys(), key=lambda x: int(x))\n",
    "    first_data = {event_id: data[event_id] for event_id in sorted_event_ids[:495]}\n",
    "    second_data = {event_id: data[event_id] for event_id in sorted_event_ids[495:500]}\n",
    "\n",
    "    # Save separated data to pickle files\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_train.pkl', 'wb') as pf1:\n",
    "        pickle.dump(first_data, pf1)\n",
    "        \n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_posterior.pkl', 'wb') as pf2:\n",
    "        pickle.dump(second_data, pf2)\n",
    "\n",
    "    check_file_length(f'{dataset.split(\".p\")[0]}_train.pkl', 495)\n",
    "    check_file_length(f'{dataset.split(\".p\")[0]}_posterior.pkl', 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the training of the emulators for all datasets\n",
    "\n",
    "After training the emulators, we save them with `dill`, such that they can be reloaded from file for the MCMC later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training without the `parameterTrafoPCA` (for $\\zeta/s(T)$, $\\eta/s(\\mu_B)$, $\\langle y_{\\rm loss}\\rangle(y_{\\rm init})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][emulator_BAND] loading training data from ./output/hadrons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator_BAND] loading training data from ./output/hadrons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] loading training data from ./output/hadrons_train.pkl ...\n",
      "[INFO][emulator] All training data are loaded.\n",
      "[INFO][emulator] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator] Performing PCA ...\n",
      "[INFO][emulator] 4 PCs explain 0.91426 of variance\n",
      "[INFO][emulator] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] GP 0: 0.53858 of variance, LML = -0.73738, Score = 0.98, kernel: 5.8**2 * RBF(length_scale=[2.61, 6.08, 2.05, 2.57, 19.5, 6.99]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 1: 0.25080 of variance, LML = -298.91, Score = 0.93, kernel: 1.17**2 * RBF(length_scale=[0.592, 0.184, 1.86, 0.611, 1.71, 0.658]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 2: 0.06666 of variance, LML = -214.01, Score = 0.95, kernel: 1.13**2 * RBF(length_scale=[0.567, 0.262, 1.6, 0.435, 3.99, 1.34]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 3: 0.05822 of variance, LML = -328.86, Score = 0.94, kernel: 1.09**2 * RBF(length_scale=[0.567, 0.168, 0.998, 0.311, 1.77, 0.914]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator_BAND] loading training data from ./output/pions_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator_BAND] loading training data from ./output/pions_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] loading training data from ./output/pions_train.pkl ...\n",
      "[INFO][emulator] All training data are loaded.\n",
      "[INFO][emulator] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator] Performing PCA ...\n",
      "[INFO][emulator] 4 PCs explain 0.99413 of variance\n",
      "[INFO][emulator] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] GP 0: 0.53257 of variance, LML = -42.101, Score = 0.98, kernel: 5.82**2 * RBF(length_scale=[1.78, 3.88, 3.98, 4.52, 2.74, 1.04]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 1: 0.40340 of variance, LML = -61.458, Score = 0.96, kernel: 4.19**2 * RBF(length_scale=[3.36, 5.07, 2.35, 6.31, 2.08, 0.915]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 2: 0.04947 of variance, LML = -97.503, Score = 0.96, kernel: 3.47**2 * RBF(length_scale=[1.38, 0.753, 2.36, 1.24, 50, 40]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 3: 0.00869 of variance, LML = -247.36, Score = 0.94, kernel: 1.07**2 * RBF(length_scale=[0.537, 0.246, 1.8, 0.443, 1.5, 1.07]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator_BAND] loading training data from ./output/kaons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator_BAND] loading training data from ./output/kaons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] loading training data from ./output/kaons_train.pkl ...\n",
      "[INFO][emulator] All training data are loaded.\n",
      "[INFO][emulator] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator] Performing PCA ...\n",
      "[INFO][emulator] 4 PCs explain 0.99586 of variance\n",
      "[INFO][emulator] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] GP 0: 0.83234 of variance, LML = -5.1716, Score = 0.98, kernel: 4.25**2 * RBF(length_scale=[7.41, 12.4, 7.74, 8.79, 0.735, 0.973]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 1: 0.12920 of variance, LML = -54.903, Score = 0.97, kernel: 3.6**2 * RBF(length_scale=[3.82, 2.72, 1.37, 1.3, 1.2, 1.8]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 2: 0.02962 of variance, LML = -270.38, Score = 0.93, kernel: 2.06**2 * RBF(length_scale=[0.773, 0.347, 1.49, 0.77, 1.05, 1.79]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 3: 0.00471 of variance, LML = -358.18, Score = 0.94, kernel: 1.2**2 * RBF(length_scale=[0.481, 0.205, 1.27, 0.364, 0.645, 0.97]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator_BAND] loading training data from ./output/protons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator_BAND] loading training data from ./output/protons_train.pkl ...\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] loading training data from ./output/protons_train.pkl ...\n",
      "[INFO][emulator] All training data are loaded.\n",
      "[INFO][emulator] Training dataset size: 495, discarded points: 0\n",
      "[INFO][emulator] Performing PCA ...\n",
      "[INFO][emulator] 4 PCs explain 0.99750 of variance\n",
      "[INFO][emulator] Train GP emulators with 495 training points ...\n",
      "[INFO][emulator] GP 0: 0.92587 of variance, LML = 30.31, Score = 0.99, kernel: 2.77**2 * RBF(length_scale=[5.65, 10.7, 7.12, 30, 0.989, 0.369]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 1: 0.06055 of variance, LML = -29.414, Score = 0.99, kernel: 3.19**2 * RBF(length_scale=[3.68, 3.26, 1.22, 1.24, 1.29, 0.503]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 2: 0.00747 of variance, LML = -244.06, Score = 0.94, kernel: 2.79**2 * RBF(length_scale=[0.938, 1.68, 0.632, 0.771, 2.1, 0.462]) + WhiteKernel(noise_level=0.01)\n",
      "[INFO][emulator] GP 3: 0.00361 of variance, LML = -198.92, Score = 0.92, kernel: 1.28**2 * RBF(length_scale=[2.07, 0.501, 1.31, 0.666, 0.889, 0.36]) + WhiteKernel(noise_level=0.01)\n"
     ]
    }
   ],
   "source": [
    "model_par = 'preds/modelDesign.txt'\n",
    "path_input = './output/'\n",
    "path_output = './trained_emulators_no_PCA/'\n",
    "\n",
    "datasets_train = [\n",
    "    'hadrons_train.pkl',\n",
    "    'pions_train.pkl',\n",
    "    'kaons_train.pkl',\n",
    "    'protons_train.pkl'\n",
    "]\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets_train:\n",
    "    emu1 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCGP', logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu1.trainEmulatorAutoMask()\n",
    "    emu2 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCSK', logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu2.trainEmulatorAutoMask()\n",
    "    emu3 = Emulator(f'{path_input}{dataset}', model_par, npc = 4, logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu3.trainEmulatorAutoMask()\n",
    "\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCGP_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu1, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCSK_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu2, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_Scikit_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an 'experimental' dataset from one of the posterior points for closure testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[4:5]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1099']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1099']['obs'] = np.concatenate((event_data[0]['1099']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1099.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[2:3]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1097']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1097']['obs'] = np.concatenate((event_data[0]['1097']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1097.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the point for the test of the logarithmic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[4:5]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "# modify the datasets with the multiplicities and take the log\n",
    "datasets_to_modify = [0,1,4,5,7]\n",
    "for i in datasets_to_modify:\n",
    "    event_data[i]['1099']['obs'][0,:] = np.log(np.abs(event_data[i]['1099']['obs'][0,:]) + 1e-30)\n",
    "    event_data[i]['1099']['obs'][1,:] = np.abs(event_data[i]['1099']['obs'][1,:]/event_data[i]['1099']['obs'][0,:] + 1e-30)\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1099']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1099']['obs'] = np.concatenate((event_data[0]['1099']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1099_LOG.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate one dataset from all of the training and posterior points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "path_output = './full_data_one_pkl/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta.pkl',\n",
    "            'AuAu200_PHOBOSv2eta.pkl',\n",
    "            'AuAu200_pTvn.pkl',\n",
    "            'AuAu19p6_dNdy.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta.pkl',\n",
    "            'AuAu19p6_pTvn.pkl',\n",
    "            'AuAu7.7_dNdy.pkl',\n",
    "            'AuAu7.7_pTvn.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    sorted_keys = sorted(data.keys())\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted_keys}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "for dataset in event_data[1:]:\n",
    "    for event in sorted_keys:\n",
    "        # Get the 'obs' array for the current event\n",
    "        obs_array_new = dataset[event]['obs']\n",
    "        \n",
    "        # Extend the 'obs' array of the first dataset with the values from the others\n",
    "        event_data[0][event]['obs'] = np.concatenate((event_data[0][event]['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}all_points_all_observables.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
